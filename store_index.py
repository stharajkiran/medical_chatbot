from dotenv import load_dotenv
import os
from src.helper import load_pdf_file, filter_to_minimal_docs, text_split, download_embeddings
from pinecone import Pinecone
from pinecone import ServerlessSpec 
from langchain_pinecone import PineconeVectorStore

# load the api keys
load_dotenv()
PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')
OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')

# Extract text from PDF files
extracted_data=load_pdf_file(data='data/')

# Get a new list of Document objects containing only 'source' in metadata and the original page_content
filter_data = filter_to_minimal_docs(extracted_data)

# Split the documents into smaller chunks
text_chunks=text_split(filter_data)

# HF model embeddings
embeddings = download_embeddings()

# api to pinecone
pc_obj = Pinecone(api_key=PINECONE_API_KEY)


# Create a dense index with integrated embedding
index_name = "medical-chatbot"  # change if desired
if not pc_obj.has_index(index_name):
    pc_obj.create_index(
        name=index_name,
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )

# Target the index
index = pc_obj.Index(index_name)

# Return VectorStore initialized from documents and embeddings 
# Store the chunk embeddings generated by the model inside pinecone 
docsearch = PineconeVectorStore.from_documents(
    documents=text_chunks,
    index_name=index_name,
    embedding=embeddings, 
)